{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":673,"status":"ok","timestamp":1650965718795,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"bo-m8t1-LJ8z","outputId":"eef0b9ab-f34e-4100-eed3-f557b7957afd"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Dataset\n"]}],"source":["import sys\n","import scipy as sc\n","import scipy.io\n","import pandas as pd\n","import numpy as np\n","%cd /content/drive/MyDrive/Dataset\n","\n","def get_coco_features(split, types ):\n","    if split == 'train':\n","        data_path = '/content/drive/MyDrive/Project data/VQA/Training Data QA.pickle'\n","        if ( types == \"small\"):\n","            num_data = 40000\n","        elif (types == \"full\"):\n","            num_data = 82783\n","\n","    elif split == 'val':\n","        data_path = '/content/drive/MyDrive/Project dataset/VQA/Validation Data QA.pickle'\n","        if (types == \"small\"):\n","            num_data = 8000\n","        elif (types == \"full\"):\n","            num_data = 40504\n","    else:\n","        print('Invalid split!')\n","        sys.exit()\n","  \n","    id_map_path = '/content/drive/MyDrive/Dataset/coco_vgg_id_map.txt'\n","    features_path = '/content/drive/MyDrive/Project dataset/Data/coco/vgg_feats.mat'\n","    img_labels = pd.read_pickle(data_path)[['image_id']].drop_duplicates().values.tolist()\n","    img_ids = open(id_map_path).read().splitlines()\n","    features_struct = sc.io.loadmat(features_path)\n","\n","    id_map = {}\n","    for ids in img_ids:\n","        ids_split = ids.split()\n","        id_map[int(ids_split[0])] = int(ids_split[1])\n","\n","    VGGfeatures = features_struct['feats']\n","    nb_dimensions = VGGfeatures.shape[0]\n","    nb_images = len(img_labels)\n","    image_matrix = np.zeros((nb_images,nb_dimensions))\n","\n","    for i in range(nb_images):\n","        image_matrix[i,:] = VGGfeatures[:,id_map[img_labels[i][0]]]  \n","    image_matrix.astype('float32')\n","    return image_matrix[0:num_data]"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":81,"status":"error","timestamp":1650965719838,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"8J5mWk7JMJEw"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading image features ...\n"]},{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-85c45c527ae3\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading image features ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0msmall_img_features_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coco_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msmall_img_features_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coco_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-1-4eb8c8b7ec97\u003e\u001b[0m in \u001b[0;36mget_coco_features\u001b[0;34m(split, types)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mid_map_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dataset/coco_vgg_id_map.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfeatures_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Project dataset/Data/coco/vgg_feats.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 28\u001b[0;31m     \u001b[0mimg_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mimg_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_map_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mfeatures_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 201\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     ) as handles:\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Project data/VQA/Training Data QA.pickle'"]}],"source":["import numpy as np\n","import h5py\n","print('Loading image features ...')\n","small_img_features_train = get_coco_features('train', types = \"small\")\n","small_img_features_val = get_coco_features('val', types = \"small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1650965719788,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"xF47VnyvMKUj"},"outputs":[],"source":["%cd /content/drive/MyDrive/VQA/Preprocessed Data\n","\n","h5_feats = h5py.File('small_img_features_train.h5', 'w')\n","h5_feats.create_dataset('small_img_features_train', data = small_img_features_train)\n","h5_feats.close()\n","\n","h5_feats_val = h5py.File('small_img_features_val.h5', 'w')\n","h5_feats_val.create_dataset('small_img_features_val', data = small_img_features_val)\n","h5_feats_val.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":33,"status":"aborted","timestamp":1650965719796,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"C20GsJdnMaWO"},"outputs":[],"source":["import nltk\n","nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":37,"status":"aborted","timestamp":1650965719800,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"I0zqwvE2MeDW"},"outputs":[],"source":["from scipy import io\n","import operator\n","import sys\n","import scipy as sc\n","from collections import defaultdict\n","from nltk import word_tokenize\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import h5py\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":38,"status":"aborted","timestamp":1650965719802,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"PLXtVqZcMgUH"},"outputs":[],"source":["def get_question_tokenizer(types):\n","    data_path = \"Training Data QA.pickle\"\n","    data_path_val = \"Validation Data QA.pickle\"\n","\n","    if ( types == \"small\"):\n","        num_data = 120000\n","        num_data_val = 24000\n","    elif (types == \"full\"):\n","        num_data = 248349\n","        num_data_val = 121512\n","\n","    df = pd.read_pickle(data_path) \n","    df_val = pd.read_pickle(data_path_val)\n","    questions = df['questions'].values.tolist()\n","    questions_val = df_val['questions'].values.tolist()\n","   \n","    all_question = questions + questions_val\n","  \n","    tokenizer = Tokenizer(num_words = 10000)\n","    tokenizer.fit_on_texts(all_question)\n","\n","    word_index = tokenizer.word_index\n","\n","    # Save the tokenizer, so that we can use this tokenizer whenever we need to predict any reviews.\n","    with open('tokenizer.pickle', 'wb') as handle:\n","        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    #tokenising train data\n","    train_question_tokenized = tokenizer.texts_to_sequences(questions)      \n","    questions = pad_sequences(train_question_tokenized, maxlen = 25)          # len(X_train) x 25\n","\n","    #tokenising validation data\n","    val_question_tokenized = tokenizer.texts_to_sequences(questions_val)\n","    questions_val = pad_sequences(val_question_tokenized, maxlen = 25)               # len(X_val) X 25 \n","\n","    return questions[0:num_data], questions_val[0: num_data_val], word_index\n","  \n","def get_questions_matrix(split):\n","  \n","    if split == 'train':\n","        data_path = 'data_train_qa.pickle'\n","    elif split == 'val':\n","        data_path = 'data_val_qa.pickle'\n","    else:\n","        print('Invalid split!')\n","        sys.exit()\n","\n","    df = pd.read_pickle(data_path)\n","    questions = df[['questions']].values.tolist()\n","    word_idx = load_idx()\n","    seq_list = []\n","\n","    for question in questions:\n","        words = word_tokenize(question[0])\n","        seq = []\n","        for word in words:\n","            seq.append(word_idx.get(word,0))\n","        seq_list.append(seq)\n","    question_matrix = pad_sequences(seq_list)\n","  \n","    question_matrix.astype('int32')\n","    return question_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":40,"status":"aborted","timestamp":1650965719804,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"Y0JeMR8MMn7K"},"outputs":[],"source":["small_question_train_tokenize, small_question_val_tokenize, word_idx = get_question_tokenizer(types = \"small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":43,"status":"aborted","timestamp":1650965719807,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"UIKTsmwyMq7E"},"outputs":[],"source":["h5f = h5py.File('small_question_train_tokenize.h5', 'w')\n","h5f.create_dataset('small_question_train_tokenize', data=small_question_train_tokenize)\n","h5f.close()\n","\n","h5f_val = h5py.File('small_question_val_tokenize.h5', 'w')\n","h5f_val.create_dataset('small_question_val_tokenize', data=small_question_val_tokenize)\n","h5f_val.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":45,"status":"aborted","timestamp":1650965719810,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"haxroXfJMu35"},"outputs":[],"source":["import pickle\n","\n","file = open(\"/content/drive/MyDrive/VQA/Preprocessed Data/word_idx.pickle\", \"wb\")\n","pickle.dump(word_idx, file)\n","file.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1650965719812,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"a4MHpmqsMwY_"},"outputs":[],"source":["import numpy as np\n","def loadGloveModel(gloveFile, word_index):\n","    print(\"Loading Glove Model\")\n","    f = open(gloveFile,'r', encoding='utf8')\n","    embedding_index = {}\n","    print(\"Opened!\")\n","    for j, line in enumerate(f):\n","        splitLine = line.split(' ')\n","        word = splitLine[0]\n","        embedding = np.asarray(splitLine[1:], dtype='float32')\n","        embedding_index[word] = embedding\n","    \n","    print(\"Done.\",len(embedding_index),\" words loaded!\")\n","  \n","    # Now, we need to create embedding matrix.\n","    EMBEDDING_DIM = 300\n","    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","    print(embedding_matrix.shape)\n","  \n","    for word, i in word_index.items():\n","        embedding_vector = embedding_index.get(word)\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n","    \n","    return embedding_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":50,"status":"aborted","timestamp":1650965719815,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"p5HcEUQjMzcI"},"outputs":[],"source":["gloveFile = '/content/drive/MyDrive/Project dataset/glove.840B.300d.txt'\n","file = open(\"/content/drive/MyDrive/VQA/Preprocessed Data/word_idx.pickle\", \"rb\")\n","word_idx = pickle.load(file)\n","file.close()\n","embedding_matrix_tokenize = loadGloveModel(gloveFile, word_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":54,"status":"aborted","timestamp":1650965719819,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"bz9aU3y_M2Gm"},"outputs":[],"source":["file = open(\"/content/drive/MyDrive/VQA/Preprocessed Data/embedding_matrix_tokenize.pickle\", \"wb\")\n","pickle.dump(embedding_matrix_tokenize, file)\n","file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":55,"status":"aborted","timestamp":1650965719821,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"aakVJtmvNGx9"},"outputs":[],"source":["h5_feats = h5py.File('/content/drive/MyDrive/VQA/Preprocessed Data/embedding_matrix_tokenize.h5', 'w')\n","h5_feats.create_dataset('embedding_matrix_tokenize', data = embedding_matrix_tokenize)\n","h5_feats.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":63,"status":"aborted","timestamp":1650965719829,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"hGxbwjdbNOj1"},"outputs":[],"source":["questions = [\"what is the stripe on the train\"]\n","with open('/content/drive/MyDrive/VQA/Preprocessed Data/tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","\n","\n","train_question_tokenized = tokenizer.texts_to_sequences(questions)      \n","questions = pad_sequences(train_question_tokenized, maxlen = 30)          # len(X_train) x 30\n","questions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":65,"status":"aborted","timestamp":1650965719831,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"ceVnIuddNWNG"},"outputs":[],"source":["def int_to_answers():\n","    \n","    data_path = '/content/drive/MyDrive/VQA/Preprocessed Data/Training Data QA.pickle'\n","    df = pd.read_pickle(data_path)\n","    answers = df['multiple_choice_answer'].values.tolist()\n","    freq = defaultdict(int)\n","    for answer in answers:\n","        freq[answer[0].lower()] += 1\n","    int_to_answer = sorted(freq.items(),key=operator.itemgetter(1),reverse=True)[0:1000]\n","    int_to_answer = [answer[0] for answer in int_to_answer]\n","    return int_to_answer\n","\n","top_answers = int_to_answers()\t\n","\n","def answers_to_onehot():\n","\ttop_answers = int_to_answers()\n","\tanswer_to_onehot = {}\n","\tfor i, word in enumerate(top_answers):\n","\t\tonehot = np.zeros(1001)\n","\t\tonehot[i] = 1.0\n","\t\tanswer_to_onehot[word] = onehot\n","\treturn answer_to_onehot\n","\t\n","answer_to_onehot_dict = answers_to_onehot()\n","\n","def get_answers_matrix(split, types):\n","  \n","    if split == 'train':\n","        data_path = '/content/drive/MyDrive/VQA/Preprocessed Data/Training Data QA.pickle'\n","        if ( types == \"small\"):\n","            num_data = 120000\n","        elif (types == \"full\"):\n","            num_data = 2483490 \n","\n","    elif split == 'val':\n","        data_path = '/content/drive/MyDrive/VQA/Preprocessed Data/Training Data QA.pickle'\n","        if (types == \"small\"):\n","            num_data = 24000\n","        elif (types == \"full\"):\n","            num_data = 1215120 \n","    else:\n","        print('Invalid split!')\n","        sys.exit()\n","     \n","    df = pd.read_pickle(data_path)\n","    answers = df['multiple_choice_answer'].values.tolist()\n","    answer_matrix = np.zeros((len(answers),1001))\n","    default_onehot = np.zeros(1001)\n","    default_onehot[1000] = 1.0\n","\t\n","    for i, answer in enumerate(answers):\n","        answer_matrix[i] = answer_to_onehot_dict.get(answer[0].lower(),default_onehot)\n","\t\n","    answer_matrix.astype('int32')\n","    return answer_matrix[0:num_data]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":67,"status":"aborted","timestamp":1650965719833,"user":{"displayName":"ds205229106 M.Sc. Data Science","userId":"13283382473412012688"},"user_tz":-330},"id":"QVJpv1uGONCu"},"outputs":[],"source":["import numpy as np\n","import h5py\n","%cd /content/drive/MyDrive/VQA/Preprocessed Data\n","print('Loading answers ...')\n","small_answers_train = get_answers_matrix('train', types = \"small\") # float64\n","small_answers_val = get_answers_matrix('val', types = \"small\")\n","\n","\n","h5_ans = h5py.File('small_answers_train.h5', 'w')\n","h5_ans.create_dataset('small_answers_train', data = small_answers_train)\n","h5_ans.close()\n","\n","h5_ans_val = h5py.File('small_answers_val.h5', 'w')\n","h5_ans_val.create_dataset('small_answers_val', data = small_answers_val)\n","h5_ans_val.close()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNqLgeOv8bOCrD1AzFIEHON","mount_file_id":"1Ggq9x0wj1oB6rHB47PwJSdf0HMLT056n","name":"final _project II.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}